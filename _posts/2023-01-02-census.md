---
layout: post
title: "US census data"
subtitle: "Visiualizing the US population"
date: 2020-01-26 23:45:13 -0400
background: '/img/posts/01.jpg'
---

# 2010 US Census data

The 2010 Census collected a variety of demographic information for all the more than 300 million people in the USA.  Here we'll focus on the subset of the data selected by the Cooper Center, who produced a map of the population density and the racial makeup of the USA (http://www.coopercenter.org/demographics/Racial-Dot-Map).  Each dot in this map corresponds to a specific person counted in the census, located approximately at their residence. (To protect privacy, the precise locations have been randomized at the block level, so that the racial category can only be determined to within a rough geographic precision.) The Cooper Center website delivers pre-rendered tiles, which is fast but limited to the plotting choices they made; here we will show how to run novel analyses focusing on whatever aspects of the data that you select yourself, rendered dynamically as requested using the [datashader](https://github.com/bokeh/datashader) library.


## Load data and set up

First, let's load this data using [dask](http://dask.pydata.org).  Dask lets you work with data in a variety of formats, and supports both in-core and out-of-core operation.  In-core operation is faster, so here we'll direct dask to load the entire dataset into memory (`df.cache(cache=dict)` below), but if you have less than 16GB RAM you may want to comment out that line to enable the out-of-core support. The data have been converted into castra format, which supports quick access, but CSV could have been used (with some performance penalty).


```
import datashader as ds
import datashader.transfer_functions as tf
import dask.dataframe as dd
import numpy as np
import pandas as pd
```



```
%%time
#df = dd.from_castra('data/census.castra')
df = dd.read_hdf('data/census.hdf', key='census')
#df = df.cache(cache=dict)
```

